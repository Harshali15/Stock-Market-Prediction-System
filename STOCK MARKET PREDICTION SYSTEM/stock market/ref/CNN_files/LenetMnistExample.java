if ( 'prettyPrint' in window ) {} else {
    document.write( '<script type="text/javascript" src="https://gist-it.appspot.com/assets/prettify/prettify.js"></script>' );
}


document.write( '<link rel="stylesheet" href="https://gist-it.appspot.com/assets/embed.css"/>' );


document.write( '<link rel="stylesheet" href="https://gist-it.appspot.com/assets/prettify/prettify.css"/>' );

document.write( '<div class="gist-it-gist">\n<div class="gist-file">\n    <div class="gist-data">\n        \n        <pre class="prettyprint">    public static void main(String[] args) throws Exception {\n        int nChannels = 1; // Number of input channels\n        int outputNum = 10; // The number of possible outcomes\n        int batchSize = 64; // Test batch size\n        int nEpochs = 1; // Number of training epochs\n        int iterations = 1; // Number of training iterations\n        int seed = 123; //\n\n        /*\n            Create an iterator using the batch size for one iteration\n         */\n        log.info("Load data....");\n        MnistDownloader.download(); //Workaround for download location change since 0.9.1 release\n        DataSetIterator mnistTrain = new MnistDataSetIterator(batchSize,true,12345);\n        DataSetIterator mnistTest = new MnistDataSetIterator(batchSize,false,12345);\n\n        /*\n            Construct the neural network\n         */\n        log.info("Build model....");\n\n        // learning rate schedule in the form of &lt;Iteration #, Learning Rate&gt;\n        Map&lt;Integer, Double&gt; lrSchedule = new HashMap&lt;&gt;();\n        lrSchedule.put(0, 0.01);\n        lrSchedule.put(1000, 0.005);\n        lrSchedule.put(3000, 0.001);\n\n        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n                .seed(seed)\n                .iterations(iterations) // Training iterations as above\n                .regularization(true).l2(0.0005)\n                /*\n                    Uncomment the following for learning decay and bias\n                 */\n                .learningRate(.01)//.biasLearningRate(0.02)\n                /*\n                    Alternatively, you can use a learning rate schedule.\n\n                    NOTE: this LR schedule defined here overrides the rate set in .learningRate(). Also,\n                    if you\'re using the Transfer Learning API, this same override will carry over to\n                    your new model configuration.\n                */\n                .learningRateDecayPolicy(LearningRatePolicy.Schedule)\n                .learningRateSchedule(lrSchedule)\n                /*\n                    Below is an example of using inverse policy rate decay for learning rate\n                */\n                //.learningRateDecayPolicy(LearningRatePolicy.Inverse)\n                //.lrPolicyDecayRate(0.001)\n                //.lrPolicyPower(0.75)\n                .weightInit(WeightInit.XAVIER)\n                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n                .updater(Updater.NESTEROVS) //To configure: .updater(new Nesterovs(0.9))\n                .list()\n                .layer(0, new ConvolutionLayer.Builder(5, 5)\n                        //nIn and nOut specify depth. nIn here is the nChannels and nOut is the number of filters to be applied\n                        .nIn(nChannels)\n                        .stride(1, 1)\n                        .nOut(20)\n                        .activation(Activation.IDENTITY)\n                        .build())\n                .layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n                        .kernelSize(2,2)\n                        .stride(2,2)\n                        .build())\n                .layer(2, new ConvolutionLayer.Builder(5, 5)\n                        //Note that nIn need not be specified in later layers\n                        .stride(1, 1)\n                        .nOut(50)\n                        .activation(Activation.IDENTITY)\n                        .build())\n                .layer(3, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n                        .kernelSize(2,2)\n                        .stride(2,2)\n                        .build())\n                .layer(4, new DenseLayer.Builder().activation(Activation.RELU)\n                        .nOut(500).build())\n                .layer(5, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n                        .nOut(outputNum)\n                        .activation(Activation.SOFTMAX)\n                        .build())\n                .setInputType(InputType.convolutionalFlat(28,28,1)) //See note below\n                .backprop(true).pretrain(false).build();\n</pre>\n        \n    </div>\n    \n    <div class="gist-meta">\n        \n        <span><a href="https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/convolution/LenetMnistExample.java">This Gist</a> brought to you by <a href="https://gist-it.appspot.com">gist-it</a>.</span>\n        \n        <span style="float: right; color: #369;"><a href="https://github.com/deeplearning4j/dl4j-examples/raw/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/convolution/LenetMnistExample.java">view raw</a></span>\n        <span style="float: right; margin-right: 8px;">\n            <a style="color: rgb(102, 102, 102);" href="https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/convolution/LenetMnistExample.java">dl4j-examples/src/main/java/org/deeplearning4j/examples/convolution/LenetMnistExample.java</a></span>\n            <!-- Generated by: https://gist-it.appspot.com -->\n    </div>\n    \n</div>\n</div>' );

document.write( '<script type="text/javascript">prettyPrint();</script>' );